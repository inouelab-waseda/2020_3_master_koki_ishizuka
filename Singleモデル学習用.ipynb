{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シングルネットワーク用のnotebook\n",
    "精度を求めるために基本的にはデュアルのネットワークを利用すれば良いが、比較のために用意している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "# import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pip/koki_ishizuka/.conda/envs/py35-zukapy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Layer,Lambda\n",
    "from keras.layers import Flatten,BatchNormalization\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imheight = 128\n",
    "imwidth = 128\n",
    "channels = 3\n",
    "ALPHA = 0.1\n",
    "BETA = 0.05\n",
    "dense_num = 512\n",
    "vec_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pip/koki_ishizuka/.conda/envs/py35-zukapy/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "#include_top=false => Dense不要\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_tensor=Input(shape=(imwidth, imheight, channels)), input_shape=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embNet():\n",
    "    x = base_model.output\n",
    "    conv1 = Conv2D(32, (4,4) , padding='same', activation='relu')(x)\n",
    "    conv2 = Conv2D(32, (4,4) , padding='same', activation='relu')(conv1)\n",
    "    flatten = Flatten()(conv2) \n",
    "    dense_layer = Dense(dense_num, activation='relu')(flatten)\n",
    "    norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n",
    "    return  Model(inputs=[base_model.input], outputs=norm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define three Inputs\n",
    "a_in = Input(shape = (imheight, imwidth, channels), name='anchor_input')\n",
    "p_in = Input(shape = (imheight, imwidth, channels), name='positive_input')\n",
    "n_in = Input(shape = (imheight, imwidth, channels), name='negative_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習後にベクトルのencoderとして利用するので外に定義する。\n",
    "con_embNet = create_embNet()\n",
    "shop_embNet = create_embNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_emb = shop_embNet(a_in)\n",
    "p_emb = con_embNet(p_in)\n",
    "n_emb = con_embNet(n_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'alpha': self.alpha}\n",
    "        base_config = super(TripletLossLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, beta, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        super(ImprovedTripletLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        pn_dist = K.sum(K.square(p-n), axis=-1)\n",
    "        return K.sum(K.maximum((p_dist - n_dist + self.alpha), 0) + K.maximum((p_dist - self.beta), 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'alpha': self.alpha}\n",
    "        base_config = super(TripletLossLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- triplet lossの選択を忘れずに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "# triplet_loss_layer = TripletLossLayer(alpha=ALPHA,name='triplet_loss_layer')([a_emb, p_emb, n_emb])\n",
    "triplet_loss_layer = ImprovedTripletLossLayer(alpha=ALPHA,beta=BETA,name='triplet_loss_layer')([a_emb, p_emb, n_emb])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "tripletNet = Model([a_in, p_in, n_in], triplet_loss_layer)\n",
    "\n",
    "# complie\n",
    "tripletNet.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = './dataset/T_Shirt_all/'\n",
    "ids = sorted([x for x in os.listdir(BASE_PATH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id_00000001'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6155"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tripletのパスの組を返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "def get_triplets(ids,BASE_PATH):\n",
    "    triplets=[]\n",
    "    for id_ in tqdm(ids):\n",
    "        files = sorted([BASE_PATH+id_+'/'+x for x in os.listdir(BASE_PATH+id_)])\n",
    "        con = sorted([x for x in files if 'comsumer' in x])\n",
    "        shop = sorted([x for x in files if 'shop' in x ])\n",
    "        combs = list(itertools.product(tuple(con),tuple(shop)))\n",
    "        for comb in combs:\n",
    "            comb = list(comb)\n",
    "            neg_id = random.choice([x for x in ids if x != id_])\n",
    "            neg_file = random.choice([BASE_PATH+neg_id+'/'+x for x in os.listdir(BASE_PATH+neg_id) if 'shop' in x])\n",
    "            comb.append(neg_file)\n",
    "            triplets.append(comb)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id単位でtrainとtestを分割する\n",
    "- 元々np.random.choice()でやっていたが、ブートストラップサンプリングだったのでダメ\n",
    "- train_test_splitを利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pip/koki_ishizuka/.conda/envs/py35-zukapy/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_stateを固定しておく\n",
    "train_ids,test_ids=train_test_split(ids,test_size=0.33,random_state=0)\n",
    "del ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_00016780', 'id_00007427', 'id_00029554', 'id_00020254', 'id_00018517']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def get_np_triplets(triplet_PATHs):\n",
    "    triplets = []\n",
    "    for triplet in tqdm(triplet_PATHs):\n",
    "        anc_img = Image.open(triplet[0]).convert('RGB')\n",
    "        pos_img = Image.open(triplet[1]).convert('RGB')\n",
    "        neg_img = Image.open(triplet[2]).convert('RGB')\n",
    "\n",
    "        anc_img = np.array(anc_img.resize((128,128)))/255. #resize to (128,128,3)\n",
    "        pos_img = np.array(pos_img.resize((128,128)))/255.    \n",
    "        neg_img = np.array(neg_img.resize((128,128)))/255.    \n",
    "\n",
    "        tri = [anc_img,pos_img,neg_img]\n",
    "        triplets.append(np.array(tri))\n",
    "    triplets = np.array(triplets)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_idsはretrival.ipynbで参照するのでpickleとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('./pickle/test_ids.pickle', 'wb')\n",
    "pickle.dump(test_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "model_dir ='./model/T_Shirt/Single/a{}b{}'.format(ALPHA,BETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各エポックでtestデータを用いてN-top accを出したい→けど結果としてはいらない？\n",
    "- epochの外でtrain,testに分割するパターンなので常にtestのidは同じ\n",
    "- 5epochに一度tripletを更新する\n",
    "- model.fitはepochs=1で行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/4123 [00:00<00:18, 220.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4123/4123 [00:21<00:00, 194.82it/s]\n",
      "100%|██████████| 38378/38378 [04:08<00:00, 154.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pip/koki_ishizuka/.conda/envs/py35-zukapy/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 164s 4ms/step - loss: 4.1963\n",
      "epoch 1\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 2.7032\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 2.2367\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 1.8646\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 1.5630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/4123 [00:00<00:21, 193.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4123/4123 [00:27<00:00, 151.98it/s]\n",
      "100%|██████████| 38378/38378 [03:30<00:00, 197.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 157s 4ms/step - loss: 1.5691\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 1.2725\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 1.0520\n",
      "epoch 8\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 0.8980\n",
      "epoch 9\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 0.7447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/4123 [00:00<00:19, 214.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4123/4123 [00:21<00:00, 191.59it/s]\n",
      "100%|██████████| 38378/38378 [03:25<00:00, 187.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 157s 4ms/step - loss: 1.0126\n",
      "epoch 11\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 0.7652\n",
      "epoch 12\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 0.6185\n",
      "epoch 13\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 0.5130\n",
      "epoch 14\n",
      "Epoch 1/1\n",
      "38378/38378 [==============================] - 156s 4ms/step - loss: 0.4345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/4123 [00:00<00:16, 246.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4123/4123 [00:22<00:00, 186.42it/s]\n",
      " 67%|██████▋   | 25522/38378 [02:30<01:09, 183.88it/s]"
     ]
    }
   ],
   "source": [
    "model_history = []\n",
    "for epoch in range(epochs):\n",
    "    print('epoch %s'% epoch)\n",
    "    if epoch % 5 == 0:\n",
    "        if epoch != 0: del triplets\n",
    "        triplets_train_PATHs = get_triplets(train_ids,BASE_PATH)\n",
    "        triplets = get_np_triplets(triplets_train_PATHs)\n",
    "        del triplets_train_PATHs\n",
    "    # fit\n",
    "    hist = tripletNet.fit([triplets[:,0],triplets[:,1],triplets[:,2]], epochs=1, batch_size=50)\n",
    "    model_history.append(hist.history)\n",
    "    f = open(model_dir+'/{}/history{}.txt'.format(vec_length,epoch),'wb')\n",
    "    pickle.dump(model_history, f)\n",
    "    # 使い終わったので削除\n",
    "##    del triplets\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        shop_embNet.save(model_dir+'/{}/shop_e{}.h5'.format(vec_length,epoch))\n",
    "        con_embNet.save(model_dir+'/{}/con_e{}.h5'.format(vec_length,epoch))\n",
    "# 学習のhistoryを保存\n",
    "f = open(model_dir+'/{}/history.txt'.format(vec_length),'wb')\n",
    "pickle.dump(model_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     print('epoch %s'% epoch)\n",
    "#     # PATHの組みを取得\n",
    "#     triplets_train_PATHs = get_triplets(train_ids,BASE_PATH)\n",
    "#     # np配列に変換\n",
    "#     triplets = get_np_triplets(triplets_train_PATHs)\n",
    "#     # fit\n",
    "#     tripletNet.fit([triplets[:,0],triplets[:,1],triplets[:,2]], epochs=1, batch_size=50) # using batch_size is better\n",
    "#     # 使い終わったので削除\n",
    "#     del triplets\n",
    "#     if (epoch+1) % 5 == 0:\n",
    "# #         tripletNet.save('./model/T_Shirt/Single/tripletNetmodel_e{}.h5'.format(epoch))\n",
    "#         # 5epochごとにmodelを保存\n",
    "#         shop_embNet.save('./model/T_Shirt/Single/a{}/{}/shop_emb_e{}.h5'.format(ALPHA,vec_length,epoch))\n",
    "#         con_embNet.save('./model/T_Shirt/Single/a{}/{}/con_emb_e{}.h5'.format(ALPHA,vec_length,epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35-zukapy",
   "language": "python",
   "name": "py35-zukapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
